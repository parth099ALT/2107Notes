# Locality and Memory Hierarchy 
![[nice_pyramid_mem_hire.png]]

The closer we get to the top the faster the memory is. 
The caches(L1, L2, L3) work in unison:

CPU registers will hold `words` retrieved from L1 which has retrieved from L2 which has retrieved from L3.

Recall what a `word` is: **2** bytes

Now L3 has retrieved from main memory which has retrieved from secondary storage(s). 

> **Note**: Any item before memory has DMA by the CPU (direct memory access). Anything below Main Memory RAM requires permissions and is indirect access. (Virtual Memory will be discussed in 3207)

The caches are SRAM while main memory is DRAM. 

## Cache Hierarchy
![[i7_caches.png]]

See that each core has its own components (registers, caches)

## Access Time Perspective
![[access_time.png]]

The left table represents a fake CPU time while the right side is scaled time. 

## Locality 
```c
for(i = 0; i < SIZE; i++){
	sum += A[i];
}
```

Variables: `i`, `sum`, `A`

Notice that `i` and `sum` is altered `size` times in total while each array element is only accessed *once*. 

Thus, `i` and `sum` are stored with better temporal locality (maybe in the cache). 
However the array will be stored with bad temporal locality but good spatial so it can be accessed faster but not to take up previous cache space. It will end up in the RAM likely. 

## Cache Sizes
Each cache has an *array* of sets where each set contains one or more lines. Each line contains a valid bit some tags and then a block of data. 

![[example_cache_sets.png]]

We can think of each cache as a building:

+ S: Floors
+ E: Number of Rooms per floor
+ B: Seats per Room

Why are sets in power of 2? So we can address them in bits. 
This addressing is called "Set Index bits", the amount of bits required is that power of 2 for the number of sets.

This also works for B. This is known as block offset bits.

## Practice Problems For *Locality*

# Caching
## 2D array example
Snippet 1
```c
sum = 0
for(i = 0; i < NROWS; i++)
	for(j = 0; j < NROWS; j++)
		sum += A[i][j]
```

Snippet 2
```c
sum = 0
for(i = 0; i < NROWS; i++)
	for(j = 0; j < NROWS; j++)
		sum += A[j][i]
```

The only difference here is the array access `A[i][j]` vs `A[j][i]`
One is a row by col traversal while one is a col by row traversal. 

Snippet 1 is *faster*. This is because we can cache the array and visit it fully before moving to the next array. 

## Cache Hit and Miss
Suppose that reading from slow memory is 100x slower than reading from a faster memory. 

+ A hit takes 1x access time 
+ A miss takes 100x access time

A hit is when a CPU finds its next (proper) target in the Cache. Having to look for it in the memory is very slow. 

**Average Access Time**
```
avg = p(hit) * hit_time + p(miss) * miss_time
```
It looks like an expected value calculation. 

Suppose `p(hit) = .97`
Then, 

```c
avg = (0.97) * 1 + (0.03) * (1 + 100)
avg = 4 units
```

Notice how it says `1+100` and not just `100`. We `+1` since we have wasted that `1x` time from not hitting the first time. 

If our `p(hit) = .99` the average time for access will be *2 units*.

In reality, to increase cache hit rates we must increase cache sizes. 

<!-- 
This marks the end of CIS2107
Ended at 12:05 on Monday 4_25_22  
-->